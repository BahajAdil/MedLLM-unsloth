{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps trl peft accelerate bitsandbytes datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre -U xformers"
      ],
      "metadata": {
        "id": "hZTtlh5kaqT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone  https://github.com/XZhang97666/AlpaCare.git"
      ],
      "metadata": {
        "id": "ijoscOv25Urx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv '/content/AlpaCare/data' '/content'"
      ],
      "metadata": {
        "id": "xZ5Vsfra5XjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "import json\n",
        "from datasets import Dataset\n",
        "from operator import itemgetter\n",
        "from os.path import join\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# add your token\n",
        "HF_TOKEN = ''"
      ],
      "metadata": {
        "id": "Qf6aJVpgmlUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "HSJ8B1JlMVVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "hKVbt4fPPhD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_json(file_name):\n",
        "    with open(file_name) as f:\n",
        "        d = json.load(f)\n",
        "    return d"
      ],
      "metadata": {
        "id": "IjsJ0renMWoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model definition"
      ],
      "metadata": {
        "id": "XMYseIM5rSWO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "model_name = 'unsloth/gemma-7b-bnb-4bit'\n",
        "data_name = 'medinstruct-52k'\n",
        "hf_save_model_name = 'adlbh/gemma-7b-medinstruct-52k'\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MedInstruct-52k"
      ],
      "metadata": {
        "id": "kJFr2jo-K10R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.eos_token"
      ],
      "metadata": {
        "id": "FC-EaDkq8cyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.bos_token"
      ],
      "metadata": {
        "id": "WaeLt_2V8glR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instruction = examples[\"instruction\"]\n",
        "    input       = examples[\"input\"]\n",
        "    output      = examples[\"output\"]\n",
        "    text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "    return { \"text\" : text, }\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
        "# dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_test_prompts(X_train, X_eval):\n",
        "    X_train = pd.DataFrame(X_train.apply(formatting_prompts_func, axis=1),\n",
        "                          columns=[\"text\"])\n",
        "    X_eval = pd.DataFrame(X_eval.apply(formatting_prompts_func, axis=1),\n",
        "                          columns=[\"text\"])\n",
        "    # X_test = pd.DataFrame(X_test.apply(generate_prompt('test'), axis=1), columns=[\"text\"])\n",
        "\n",
        "    train_data = Dataset.from_pandas(X_train)\n",
        "    eval_data = Dataset.from_pandas(X_eval)\n",
        "    # test_data = Dataset.from_pandas(X_test)\n",
        "    return train_data, eval_data\n",
        "\n",
        "\n",
        "def get_med_instruct_data():\n",
        "\n",
        "    med_inst_data = read_json('/content/data/MedInstruct-52k.json')\n",
        "    print('Alpacare Instruction Data size:', len(med_inst_data))\n",
        "    print(type(med_inst_data))\n",
        "\n",
        "    med_inst_train, med_inst_val, y_train, y_val = train_test_split(np.array(range(len(med_inst_data))), [0]*len(med_inst_data), test_size=0.20, random_state=42)\n",
        "    med_inst_train, med_inst_val = med_inst_train.tolist(), med_inst_val.tolist()\n",
        "    med_inst_train, med_inst_val = itemgetter(*med_inst_train)(med_inst_data), itemgetter(*med_inst_val)(med_inst_data)\n",
        "\n",
        "    med_inst_train = pd.DataFrame(med_inst_train)\n",
        "    med_inst_val = pd.DataFrame(med_inst_val)\n",
        "\n",
        "    train_data = Dataset.from_pandas(med_inst_train)\n",
        "    val_data = Dataset.from_pandas(med_inst_val)\n",
        "    train_data = train_data.map(formatting_prompts_func)\n",
        "    val_data = val_data.map(formatting_prompts_func)\n",
        "\n",
        "    print('train size: ', len(med_inst_train))\n",
        "    print('val size: ', len(med_inst_val))\n",
        "    print('med_inst_train.columns: ', med_inst_train.columns)\n",
        "    # train_data, val_data = get_train_test_prompts(med_inst_train, med_inst_val)\n",
        "\n",
        "    return train_data, val_data\n",
        "\n"
      ],
      "metadata": {
        "id": "p_xhvTrZ4jXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "med_train, med_val = get_med_instruct_data()"
      ],
      "metadata": {
        "id": "PTx2RBeW5iMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "med_train[0]"
      ],
      "metadata": {
        "id": "h5bZ6SY0W3NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ],
      "metadata": {
        "id": "idAEIeSQ3xdS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = med_train,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 8,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 1000,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(hf_save_model_name, token = HF_TOKEN) # Online saving\n",
        "tokenizer.push_to_hub(hf_save_model_name, token = HF_TOKEN) # Online saving"
      ],
      "metadata": {
        "id": "NNB-8bOrMwGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload and Inference"
      ],
      "metadata": {
        "id": "JY_CAPfDeJ-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_model(query, model, tokenizer):\n",
        "    inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            query, # instruction\n",
        "            \"\", # input\n",
        "            \"\", # output - leave this blank for generation!\n",
        "        )\n",
        "    ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "    prompt_length = inputs['input_ids'].shape[1]\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 1000, use_cache = True)\n",
        "    # answer = tokenizer.batch_decode(outputs)\n",
        "    answer = tokenizer.decode(outputs[0][prompt_length:])\n",
        "    return answer\n",
        "\n"
      ],
      "metadata": {
        "id": "76PPrcNneVyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = hf_save_model_name, # YOUR MODEL YOU USED FOR TRAINING\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# alpaca_prompt = You MUST copy from above!\n",
        "\n"
      ],
      "metadata": {
        "id": "SPmlTdbueMku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infer_model('how to treat asthma', model, tokenizer)"
      ],
      "metadata": {
        "id": "a5IS7kigmM5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PdOkb9WImS5C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}